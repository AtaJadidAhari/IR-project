{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ98SbQMJneB"
   },
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "import re\n",
    "\n",
    "def remove_extras(word):\n",
    "    to_remove = ['=', '<ref>', '</ref>', '<small>', '<font>', '</font>' , '<span>', '</span>', '</small>', '>', '<' , '\"', \"'\", '&nbsp;', '*', ';']\n",
    "    \n",
    "    removed = word\n",
    "    for i in to_remove:\n",
    "        removed = removed.replace(i, '')\n",
    "    return removed\n",
    "\n",
    "\n",
    "def my_split(word):\n",
    "    return re.split('\\||-|_|\\/|', word)\n",
    "\n",
    "def get_words(raw_text):\n",
    "    prepared_text = raw_text\n",
    "    \n",
    "    #Nomralize\n",
    "    normalizer = Normalizer()\n",
    "    prepared_text = normalizer.normalize(prepared_text)\n",
    "    \n",
    "    #Tokenize\n",
    "    prepared_text = word_tokenize(prepared_text)\n",
    "    \n",
    "    #Remove Punctuations\n",
    "    punctuations = ['.', '!', '،', '؛', '}', '{', ']', '[', '=', '*', '+', ':', '\"', \"'\"]\n",
    "    \n",
    "    prepared_text = [remove_extras(i) for i in prepared_text if i not in punctuations]\n",
    "    \n",
    "    return prepared_text\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    prepared_text = []\n",
    "    \n",
    "    raw_text = re.split('\\||-|_|', raw_text)\n",
    "    i = 0\n",
    "    for t in raw_text:\n",
    "            \n",
    "        prepared_text +=  get_words(t)\n",
    "    \n",
    "    #Stemming\n",
    "    stemmer = Stemmer()\n",
    "    prepared_text = [stemmer.stem(i) for i in prepared_text]\n",
    "    return prepared_text\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "#print('Enter text:')\n",
    "#raw_text = input()\n",
    "#print(prepare_text(raw_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_akK-pvJneI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "un_prepared_words = set()\n",
    "def add_document_to_index(doc, index):\n",
    "    doc_title = doc['title']\n",
    "    doc_id = doc['id']\n",
    "    doc_text = doc['text']\n",
    "   \n",
    "    for i in range(len(doc['title'])):\n",
    "        #print(dictionary_index)\n",
    "        if doc_title[i] in index.keys():\n",
    "           \n",
    "            if doc_id in index[doc_title[i]].keys():\n",
    "                if 'title' in index[doc_title[i]][doc_id]:\n",
    "                    \n",
    "                    index[doc_title[i]][doc_id]['title'].append(i + 1)\n",
    "                else:\n",
    "                    index[doc_title[i]][doc_id]['title'] = [i + 1]\n",
    "            else:\n",
    "                index[doc_title[i]][doc_id] = dict()\n",
    "                index[doc_title[i]][doc_id]['title'] = [i + 1]\n",
    "        else:\n",
    "            index[doc_title[i]] = dict()\n",
    "            index[doc_title[i]][doc_id] = dict()\n",
    "            index[doc_title[i]][doc_id]['title'] = [i + 1]\n",
    "    for i in range(len(doc['text'])):\n",
    "        #print(dictionary_index)\n",
    "        if doc_text[i] in index.keys():\n",
    "           \n",
    "            if doc_id in index[doc_text[i]].keys():\n",
    "                if 'text' in index[doc_text[i]][doc_id]:\n",
    "                    \n",
    "                    index[doc_text[i]][doc_id]['text'].append(i + 1)\n",
    "                else:\n",
    "                    index[doc_text[i]][doc_id]['text'] = [i + 1]\n",
    "            else:\n",
    "                index[doc_text[i]][doc_id] = dict()\n",
    "                index[doc_text[i]][doc_id]['text'] = [i + 1]\n",
    "        else:\n",
    "            index[doc_text[i]] = dict()\n",
    "            index[doc_text[i]][doc_id] = dict()\n",
    "            index[doc_text[i]][doc_id]['text'] = [i + 1]\n",
    "        \n",
    "    return index\n",
    "\n",
    "def construct_positional_indexes(docs_path):\n",
    "\n",
    "    tree = ET.parse(docs_path  + '/'+ 'Persian.xml')\n",
    "    root = tree.getroot() \n",
    "    \n",
    "    data = [] \n",
    "    \n",
    "    for child in root.iter('{http://www.mediawiki.org/xml/export-0.10/}page'):\n",
    "        title_text = next(child.iter('{http://www.mediawiki.org/xml/export-0.10/}title')).text\n",
    "        splited = re.split('\\||-|_|', title_text)\n",
    "        for txt in splited:\n",
    "            for w in get_words(txt):\n",
    "                un_prepared_words.add(w)\n",
    "        text_text = next(child.iter('{http://www.mediawiki.org/xml/export-0.10/}text')).text\n",
    "        splited = re.split('\\||-|_|', text_text)\n",
    "        for txt in splited:\n",
    "            for w in get_words(txt):\n",
    "                un_prepared_words.add(w)\n",
    "        \n",
    "        \n",
    "        new_data = {'id': 0, 'title': '', 'text': ''}\n",
    "        new_data['title'] = prepare_text(title_text)\n",
    "        new_data['id'] = int(next(child.iter('{http://www.mediawiki.org/xml/export-0.10/}id')).text)\n",
    "        \n",
    "        new_data['text'] = prepare_text(text_text)\n",
    "        \n",
    "        data.append(new_data)\n",
    "        \n",
    "    index = dict()\n",
    "    for doc in data:\n",
    "        index = add_document_to_index(doc, index)\n",
    "    return index, data\n",
    "\n",
    "index, db = construct_positional_indexes('data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLnvKnQ0JneN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6881: {'text': [500]}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_posting_list(word):\n",
    "    \n",
    "    word = prepare_text(word)\n",
    "    \n",
    "    if word[0] not in index:\n",
    "        print('we do not have this word in index!')\n",
    "        return\n",
    "        \n",
    "    posting_list = index[word[0]]\n",
    "   \n",
    "    return posting_list\n",
    "\n",
    "get_posting_list('انگلولاتین')\n",
    "get_posting_list('جایاانکول')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6qxX9KAJneT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def construct_bigram_index(words):\n",
    "    bigram_index = dict()\n",
    "    for word in words:\n",
    "        indicated_word = '$' + word + '$'\n",
    "        for i in range(len(indicated_word) - 1):\n",
    "            bigram = indicated_word[i : i + 2]\n",
    "            if bigram in bigram_index.keys():\n",
    "                bigram_index[bigram].append(word)\n",
    "                \n",
    "            else:\n",
    "                bigram_index[bigram] = [word]\n",
    "     \n",
    "    for b in bigram_index:\n",
    "        bigram_index[b].sort()\n",
    "    return bigram_index\n",
    "\n",
    "\n",
    "def get_words_with_bigram(bigram):\n",
    "    words = bigram_index[bigram]\n",
    "    return words\n",
    "\n",
    "\n",
    "bigram_index = construct_bigram_index(un_prepared_words)\n",
    "get_words_with_bigram('اا')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3V8mUFHJneZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have this data in index now!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_document(docs_path, doc_num):\n",
    "    url = '{http://www.mediawiki.org/xml/export-0.10/}'\n",
    "    tree = ET.parse(docs_path  + '/'+ 'Persian.xml')\n",
    "    root = tree.getroot()  \n",
    "    matchedUrl = \"./\"+ url  + \"page/[\" + url + \"id='\" + str(doc_num) + \"']\"\n",
    "    for child in root.findall(matchedUrl):  \n",
    "        \n",
    "        #print([elem.tag for elem in child.iter()])\n",
    "        un_prepared = set()\n",
    "        title_text = next(child.iter('{http://www.mediawiki.org/xml/export-0.10/}title')).text\n",
    "        splited = re.split('\\||-|_|', title_text)\n",
    "        for txt in splited:\n",
    "            for w in get_words(txt):\n",
    "                un_prepared.add(w)\n",
    "        text_text = next(child.iter('{http://www.mediawiki.org/xml/export-0.10/}text')).text\n",
    "        splited = re.split('\\||-|_|', text_text)\n",
    "        for txt in splited:\n",
    "            for w in get_words(txt):\n",
    "                un_prepared.add(w)\n",
    "        \n",
    "        new_data = {'id': 0, 'title': '', 'text': ''}\n",
    "        new_data['title'] = prepare_text(next(child.iter('{http://www.mediawiki.org/xml/export-0.10/}title')).text)\n",
    "        \n",
    "        new_data['id'] = int(next(child.iter('{http://www.mediawiki.org/xml/export-0.10/}id')).text)\n",
    "        new_data['text'] = prepare_text(next(child.iter('{http://www.mediawiki.org/xml/export-0.10/}text')).text)\n",
    "        \n",
    "        \n",
    "    return new_data, un_prepared\n",
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    \n",
    "    new_data, un_prepared = get_document(docs_path, doc_num)\n",
    "    if not new_data in db:\n",
    "          \n",
    "        new_index = add_document_to_index(new_data, index)\n",
    "        \n",
    "        db.append(new_data)\n",
    "    else:\n",
    "        print('we have this data in index now!')\n",
    "        return index\n",
    "    return new_index\n",
    "#index = add_document_to_indexes('data', 4589) \n",
    "index = add_document_to_indexes('data', 6881) \n",
    "#جایاانکول"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ_8rPAxJneg"
   },
   "outputs": [],
   "source": [
    "def find_all_words(text, title):\n",
    "    all_words = set()\n",
    "    for word in text:\n",
    "        all_words.add(word)\n",
    "    for word in title:\n",
    "        all_words.add(word)\n",
    "   \n",
    "    return all_words\n",
    "    \n",
    "\n",
    "def delete_from_bigram(words):\n",
    "    \n",
    "    for word in words:\n",
    "        pw = prepare_text(word)\n",
    "        if len(pw) > 0 and pw[0] not in index:\n",
    "           \n",
    "            indicated_word = '$' + word + '$'\n",
    "            for i in range(len(indicated_word) - 1):\n",
    "                bigram = indicated_word[i : i + 2]\n",
    "                \n",
    "                bigram_index[bigram].remove(word)\n",
    "\n",
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    \n",
    "    new_data, unprepared = get_document(docs_path, doc_num)\n",
    "    if not new_data in db:\n",
    "        print('we do not have this data in index now!')\n",
    "        return\n",
    "    else:\n",
    "        \n",
    "        \n",
    "        \n",
    "        all_words = find_all_words(new_data['text'], new_data['title'])        \n",
    "        for i in all_words:\n",
    "            del(index[i][new_data['id']])\n",
    "            if len(index[i].keys()) == 0:\n",
    "                del(index[i]) \n",
    "                \n",
    "        delete_from_bigram(unprepared)\n",
    "        \n",
    "        db.remove(new_data)\n",
    "        \n",
    "\n",
    "#delete_document_from_indexes('data', 4589)\n",
    "delete_document_from_indexes('data', 6881)\n",
    "\n",
    "#print(get_posting_list('انگولاتین'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UestOXpOJnel"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_index(destination):\n",
    "    #with open(destination ,\"w\") as f:\n",
    "        #f.write(str(index))\n",
    "    pickle_out = open(destination , 'wb')\n",
    "    pickle.dump(index, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "\n",
    "save_index('storage/index_backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahonOMFMJneq"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_index(source):\n",
    "    pickle_in = open(source, 'rb')\n",
    "    index = pickle.load(pickle_in)\n",
    "    return index\n",
    "    \n",
    "index = load_index('storage/index_backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5kybrGDJnev"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'آلام حالا برسلان ارسک شد'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def edit_distance(a, b):\n",
    "    dp = [[0 for i in range(len(a) + 1)] for j in range(len(b) + 1)]\n",
    "    \n",
    "    for i in range(1, len(a) + 1):\n",
    "        dp[0][i] = i\n",
    "    for j in range(1, len(b) + 1):\n",
    "        dp[j][0] = j\n",
    "    for j in range(1, len(b) + 1):\n",
    "        for i in range(1, len(a) + 1):\n",
    "            matching_score = 0\n",
    "            if a[i - 1] != b[j - 1]:\n",
    "                matching_score = 1\n",
    "            dp[j][i] = min(dp[j - 1][i] + 1, dp[j][i - 1] + 1, dp[j - 1][i - 1] + matching_score)\n",
    "   \n",
    "    #print(a, b, dp[-1][-1])\n",
    "    return dp[-1][-1]\n",
    "\n",
    "def edit_distance_select(query, jacard_selected):\n",
    "    current = 0\n",
    "    current_score = edit_distance(query, jacard_selected[0])\n",
    "    \n",
    "    for i in range(1, len(jacard_selected)):\n",
    "        new_score = edit_distance(query, jacard_selected[i])\n",
    "        if new_score < current_score:\n",
    "            current_score = new_score\n",
    "            current = i\n",
    "    return jacard_selected[current]\n",
    "\n",
    "def find_minim(pointers, bigrams):\n",
    "    current_words = [[bigrams[i][pointers[i]], i] for i in range(len(pointers)) if pointers[i] < len(bigrams[i]) ]\n",
    "   \n",
    "    minim = [i[1] for i in current_words if i[0] == min(current_words)[0]]\n",
    "    \n",
    "    #print(min(current_words), current_words)\n",
    "    return minim\n",
    "    \n",
    "    \n",
    "def calculate_jacard(query, word, common_bigrams):\n",
    "    jacard_distance = common_bigrams / (len(query) + len(word) + 2 - common_bigrams)\n",
    "    return jacard_distance\n",
    "\n",
    "def jacard_selected(query):\n",
    "    threshhold = 0.42\n",
    "    \n",
    "    selected_words = []\n",
    "    bigrams = []\n",
    "    indicated_word = '$' + query + '$'\n",
    "    for i in range(len(indicated_word) - 1):\n",
    "        bigrams.append(bigram_index[indicated_word[i : i + 2]])\n",
    "    pointers = [0 for i in range(len(bigrams))]\n",
    "\n",
    "    while True:\n",
    "        minim = find_minim(pointers, bigrams)\n",
    "        \n",
    "       # print(pointers[minim[0]], minim)\n",
    "     \n",
    "        jacard_distance = calculate_jacard(query, bigrams[minim[0]][pointers[minim[0]]], len(minim))\n",
    "        \n",
    "        if jacard_distance > threshhold:\n",
    "        \n",
    "            \n",
    "            selected_words.append(bigrams[minim[0]][pointers[minim[0]]])\n",
    "        for index in minim:\n",
    "            pointers[index] += 1\n",
    "        num = 0\n",
    "        for i in range(len(pointers)):\n",
    "            if pointers[i] >= len(bigrams[i]):\n",
    "                num += 1\n",
    "        \n",
    "        if num == len(bigrams):\n",
    "            break\n",
    "    return selected_words\n",
    "    \n",
    "\n",
    "\n",
    "def correct_query(query):\n",
    "    \n",
    "    \n",
    "    words = get_words(query)\n",
    "   \n",
    "    jacard_select = []\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in un_prepared_words:\n",
    "            \n",
    "            jacard_select = jacard_selected(words[i])\n",
    "            \n",
    "            replaced_word = edit_distance_select(words[i], jacard_select)\n",
    "            words[i] = replaced_word\n",
    "    \n",
    "    \n",
    "    #correct_query = \"سلام حالا پرسمان درست شد\"\n",
    "  \n",
    "    correct_query = ' '.join(words)\n",
    "    return correct_query\n",
    "#پرسمان = 0.2727272727272727\n",
    "correct_query(\"شلام حالا برسهان درسک شد\")\n",
    "#correct_query('خاورمینا')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsgz32PDJnez",
    "outputId": "b3b6ed43-794b-42e1-bc03-ee3e0134318b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "corpus_length = len(index.keys())\n",
    "def get_tf(index, word, doc_id):\n",
    "    \n",
    "    tfs = {'title': 0, 'text': 0}\n",
    "    if doc_id not in index[word]:\n",
    "        return tfs\n",
    "    else:\n",
    "        \n",
    "        for key in index[word][doc_id].keys():\n",
    "            tfs[key] = len(index[word][doc_id][key])\n",
    "    #print(tfs)\n",
    "    return tfs\n",
    "\n",
    "def get_df(index, word):\n",
    "    dfs = {'title': 0, 'text': 0}\n",
    "    \n",
    "    if word not in index.keys():\n",
    "        return dfs\n",
    "    \n",
    "    else:  \n",
    "        for doc_id in index[word]:\n",
    "            \n",
    "            for key in index[word][doc_id]:\n",
    "                dfs[key] += len(index[word][doc_id][key])\n",
    "                \n",
    "    \n",
    "    return dfs\n",
    "\n",
    "def get_ltf(tf):\n",
    "    logarithmic_tf = dict(tf)\n",
    "    for key in logarithmic_tf.keys():\n",
    "        if logarithmic_tf[key] != 0:\n",
    "            logarithmic_tf[key] = 1 + math.log(logarithmic_tf[key], 10)\n",
    "    \n",
    "    \n",
    "    return logarithmic_tf\n",
    "    \n",
    "def get_idf(df):\n",
    "    #division by zero!\n",
    "    idf = dict(df)\n",
    "    for key in idf.keys():\n",
    "        if idf[key] != 0:\n",
    "            idf[key] =  math.log(corpus_length / idf[key], 10)\n",
    "    \n",
    "    \n",
    "    return idf\n",
    "def normalizer(vector):\n",
    "    \n",
    "    norm_two = 0\n",
    "    for i in vector:\n",
    "        norm_two += i**2\n",
    "    if norm_two == 0:\n",
    "        return vector\n",
    "    norm_two = math.sqrt(norm_two)\n",
    "    normalized = [(i / norm_two) for i in vector]\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def calculate_total_score(tf, idf, weight):\n",
    "    #print(tf, idf)\n",
    "    score = 0\n",
    "    score += tf['title'] * weight * idf['title']\n",
    "    score += tf['text'] * idf['text']\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_qoutation_indexes(query):\n",
    "    qoutaton_indexes = [i for i in range(len(query)) if query[i] == '\"']\n",
    "    return qoutaton_indexes\n",
    "    \n",
    "def get_qouted_and_unqouted(query):\n",
    "    qoutation_indexes = get_qoutation_indexes(query)\n",
    "    \n",
    "    qouted = [] \n",
    "    unqouted = []\n",
    "    for i in range(len(qoutation_indexes) - 1):\n",
    "        if i % 2 == 0:\n",
    "            qouted.append(query[qoutation_indexes[i] + 1: qoutation_indexes[i + 1] + 1])\n",
    "        else:\n",
    "            unqouted.append(query[qoutation_indexes[i] + 1: qoutation_indexes[i + 1]])\n",
    "    if qoutation_indexes[0] != 0:\n",
    "      \n",
    "        unqouted.append(query[:qoutation_indexes[0]])\n",
    "    if qoutation_indexes[-1] != len(query) - 1:\n",
    "        unqouted.append(query[qoutation_indexes[-1]: ])\n",
    "    return qouted, unqouted\n",
    "\n",
    "def is_qouted(doc, qouted, search='text'):#search equals title or text\n",
    "    places = []\n",
    "    for couples in qouted:\n",
    "        prepared_couples = prepare_text(couples)\n",
    "        for word in prepared_couples:\n",
    "            if doc['id'] not in index[word].keys():\n",
    "                return False\n",
    "            if search not in index[word][doc['id']].keys():\n",
    "                return False\n",
    "            \n",
    "          \n",
    "            places.append(index[word][doc['id']][search])\n",
    "        for i in places[0]:\n",
    "            flag = True\n",
    "            for j in range(1, len(places)):\n",
    "                if i + j not in places[j]:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                return True \n",
    "    return False\n",
    "    \n",
    "def find_qouted_docs(db, qouted, search='all'):#finds docs that contain qouted parts\n",
    "    qouted_docs = []\n",
    "    if search == 'all':\n",
    "        for doc in db:\n",
    "            if is_qouted(doc, qouted, 'text') or is_qouted(doc, qouted, 'title'):\n",
    "                qouted_docs.append(doc)\n",
    "    else:\n",
    "        for doc in db:\n",
    "            if is_qouted(doc, qouted, search):\n",
    "                qouted_docs.append(doc)\n",
    "    return qouted_docs\n",
    "\n",
    "def get_selected_docs(query, search='all'):\n",
    "    selected_docs = list(db)\n",
    "    \n",
    "    if search == 'all':\n",
    "        if '\"'  in query:\n",
    "            qouted, unqouted = get_qouted_and_unqouted(query)\n",
    "          \n",
    "            selected_docs = find_qouted_docs(db, qouted, 'all')\n",
    "    else:\n",
    "        if '\"'  in query:\n",
    "            qouted, unqouted = get_qouted_and_unqouted(query)\n",
    "            selected_docs = find_qouted_docs(db, qouted, search)\n",
    "    return selected_docs\n",
    "\n",
    "def check_word_in_dictionary(word):\n",
    "    if word in index:\n",
    "        return True\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2):\n",
    " \n",
    "    selected_docs = get_selected_docs(query, 'all')\n",
    "    query_by_word = prepare_text(query)\n",
    "    w = [0 for i in range(len(query_by_word))]\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    relevant_docs = []\n",
    "    for doc in selected_docs:\n",
    "        \n",
    "        scores = []\n",
    "        for q in query_by_word:\n",
    "           \n",
    "            if q not in index:\n",
    "                score = 0\n",
    "               \n",
    "            \n",
    "            else:\n",
    "                tfs = get_tf(index, q, doc['id'])    \n",
    "                ltf = get_ltf(tfs)\n",
    "            \n",
    "\n",
    "                dfs = get_df(index, q)\n",
    "                idf = get_idf(dfs)\n",
    "            \n",
    "          \n",
    "\n",
    "\n",
    "            scores.append(calculate_total_score(ltf, idf, weight))\n",
    "        \n",
    "        if method == 'ltc-lnc':\n",
    "            scores = normalizer(scores)\n",
    "        scores = sum(scores)   \n",
    "        relevant_docs.append([scores,doc['id']])\n",
    "        relevant_docs.sort(reverse=True)\n",
    "        relevant_docs = relevant_docs[:15]\n",
    "        \n",
    "    return relevant_docs\n",
    "#print(normalizer([1.3,2,3]))\n",
    "#search('نظرخواهی انجام شده توسط دانشگاه \"شهر نیویورک\"', \"ltc-lnc\", 3)\n",
    "\n",
    "\n",
    "#search('سیاره های بزرگ \"منظومه شمسی\"', 'ltc-ln', 2)\n",
    "#search('کشورهای دارای نفت در خاورزمین', 'ltc-lnc', 2)\n",
    "#print(index['نیویورک'])\n",
    "\n",
    "                                  \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmOjg1gYJne6",
    "outputId": "88f5ed0f-3ac6-4bba-c148-8e7aa4de09c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.394651788255002, 3854],\n",
       " [1.9887278320066466, 3938],\n",
       " [1.9859230291050913, 6752],\n",
       " [1.9849580423318987, 3120],\n",
       " [1.9788123049694293, 6917],\n",
       " [1.9765801313596059, 7143],\n",
       " [1.9762403887145932, 3260],\n",
       " [1.9750469063412226, 5192],\n",
       " [1.9725214974634648, 5967],\n",
       " [1.9715655489522068, 6949],\n",
       " [1.96775977887812, 4401],\n",
       " [1.966045725654069, 4094],\n",
       " [1.9650869486204239, 7100],\n",
       " [1.9613542630191763, 5309],\n",
       " [1.958951499177989, 3667]]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_in_part(query, doc, search, method):\n",
    "    \n",
    "    \n",
    "    query_by_word = prepare_text(query)\n",
    "    w = [0 for i in range(len(query_by_word))]\n",
    "    scores = []\n",
    "    relevant_docs = []\n",
    "    \n",
    "        \n",
    "    scores = []\n",
    "    for q in query_by_word:\n",
    "           \n",
    "        if q not in index:\n",
    "            scores.append(ltf * idf)\n",
    "        else:\n",
    "            tfs = get_tf(index, q, doc['id'])    \n",
    "            ltf = get_ltf(tfs)[search]\n",
    "\n",
    "\n",
    "            dfs = get_df(index, q)\n",
    "            idf = get_idf(dfs)[search]\n",
    "\n",
    "            scores.append(ltf * idf)\n",
    "        \n",
    "    if method == 'ltc-lnc':\n",
    "        scores = normalizer(scores)\n",
    "    scores = sum(scores)   \n",
    "    return scores\n",
    "        \n",
    "\n",
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    selected_docs_title = get_selected_docs(title_query, 'title')\n",
    "    selected_docs_text = get_selected_docs(text_query, 'text') \n",
    "    selected_docs = [i for i in selected_docs_title if i in selected_docs_text]\n",
    "    \n",
    "    score = 0\n",
    "    relevant_docs = []\n",
    "    for doc in selected_docs:\n",
    "        score = search_in_part(title_query, doc, 'title', method)\n",
    "        score += search_in_part(text_query, doc, 'text', method)\n",
    "        \n",
    "        relevant_docs.append([score,doc['id']])\n",
    "        relevant_docs.sort(reverse=True)\n",
    "        relevant_docs = relevant_docs[:15]\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "detailed_search('عجایب هفت‌گانه', 'چشمگیرترین بناهای تاریخی جهان', \"ltc-lnc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqaOk4ESJnfA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is: 0.6533333333333332\n",
      "F_measure is: 0.6934880384965016\n",
      "MAP is: 0.8360511316523846\n",
      "NDCG is: 0.7714660920937972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7714660920937972"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "def get_reterived_and_relevant(query_id, method):\n",
    "    with open('./data/queries/%s.txt'%(query_id,), encoding='utf-8') as query_file:\n",
    "        query = query_file.readlines()\n",
    "        \n",
    "        if len(query) == 1:\n",
    "            #print(query[0])\n",
    "            if not '\"' in query[0]:\n",
    "                query[0] = correct_query(query[0])\n",
    "          \n",
    "            #print(query[0])\n",
    "            reterived = search(query[0], method, weight=2 ) #[score, doc_id]\n",
    "            reterived_ids = [i[1] for i in reterived]\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            reterived = detailed_search(query[0], query[1], method)\n",
    "            reterived_ids = [i[1] for i in reterived]\n",
    "    with open('./data/relevance/%s.txt'%(query_id,)) as relevance_file:\n",
    "        relevant = list(map(int, relevance_file.read().split(',')))\n",
    "    return reterived_ids, relevant\n",
    "def calculate_precision(reterived, relevant):\n",
    "    tp = len([i for i in reterived if i in relevant])\n",
    "    precision = tp / len(reterived)\n",
    "    return precision\n",
    "\n",
    "def calculate_recall(reterived, relevant):\n",
    "    tp = len([i for i in reterived if i in relevant])\n",
    "    recall = tp / len(relevant)\n",
    "    return recall\n",
    "    \n",
    "\n",
    "def R_Precision(query_id='all', method='ltn-lnn'):\n",
    "    \n",
    "    \n",
    "    result = 0\n",
    "    if query_id == 'all':\n",
    "        num_of_queries = 20\n",
    "        for i in range(1, num_of_queries + 1):\n",
    "            reterived, relevant = get_reterived_and_relevant(i, method)\n",
    "            result += calculate_precision(reterived, relevant)\n",
    "            #print(calculate_precision(reterived, relevant))\n",
    "            \n",
    "        result = result / num_of_queries\n",
    "    else:\n",
    "        reterived, relevant = get_reterived_and_relevant(query_id)\n",
    "        result = calculate_precision(reterived, relevant)\n",
    "        \n",
    "        \n",
    "                  \n",
    "    print('Precision is: ' + str(result))\n",
    "    return result\n",
    "\n",
    "def F_measure(query_id='all', method='ltn-lnn'):\n",
    "    result = 0\n",
    "    recall = 0\n",
    "    precision = 0\n",
    "    if query_id == 'all':\n",
    "        num_of_queries = 20\n",
    "        for i in range(1, num_of_queries + 1):\n",
    "            reterived, relevant = get_reterived_and_relevant(i, method)\n",
    "            precision += calculate_precision(reterived, relevant)\n",
    "            recall += calculate_recall(reterived, relevant)\n",
    "        recall = recall / num_of_queries\n",
    "        precision = precision / num_of_queries\n",
    "    else:\n",
    "        reterived, relevant = get_reterived_and_relevant(query_id)\n",
    "        precision = calculate_precision(reterived, relevant)\n",
    "        recall = calculate_recall(reterived, relevant)\n",
    "    \n",
    "    #setting alpha = 0.5, beta = 1\n",
    "    beta = 1\n",
    "    if (recall + precision) != 0: \n",
    "        result = (2 * precision * recall) / (recall + precision)\n",
    "    else:\n",
    "        result = 0\n",
    "    print('F_measure is: ' + str(result))\n",
    "    return result\n",
    "\n",
    "def calculate_map(reterived, relevant):\n",
    "    mapp = 0\n",
    "    tp = len([i for i in reterived if i in relevant])\n",
    "    for i in range(len(reterived)):\n",
    "        if reterived[i] in relevant:\n",
    "            mapp += calculate_precision(reterived[:i + 1], relevant)\n",
    "    if tp == 0:\n",
    "        return 0\n",
    "    mapp = mapp / tp\n",
    "    return mapp\n",
    "    \n",
    "def MAP(query_id='all', method='ltn-lnn'):\n",
    "    \n",
    "    result = 0\n",
    "    if query_id == 'all':\n",
    "        num_of_queries = 20\n",
    "        for i in range(1, num_of_queries + 1):\n",
    "            reterived, relevant = get_reterived_and_relevant(i, method)\n",
    "            result += calculate_map(reterived, relevant)\n",
    "        result = result / num_of_queries\n",
    "    else:\n",
    "        reterived, relevant = get_reterived_and_relevant(query_id)\n",
    "        result = calculate_map(reterived, relevant)\n",
    "                  \n",
    "    print('MAP is: ' + str(result))\n",
    "    return result\n",
    "\n",
    "def calculate_dcg(relevant, relevance_score):\n",
    "    dcg = 0\n",
    "    \n",
    "    for i in range(len(relevance_score)):\n",
    "        dcg += relevance_score[i] / (math.log(i + 2, 2))\n",
    "    return dcg\n",
    "\n",
    "def NDCG(query_id='all', method='ltn-lnn'):\n",
    "    \n",
    "    #base = 2, relevant:1 , non-relevant:0\n",
    "    result = 0\n",
    "    if query_id == 'all':\n",
    "        num_of_queries = 20\n",
    "        for i in range(1, num_of_queries + 1):\n",
    "            reterived, relevant = get_reterived_and_relevant(i, method)\n",
    "            \n",
    "            #k = length relevant, r_i > 0 for all docs in relevant\n",
    "            k = min(len(relevant), len(reterived))\n",
    "            \n",
    "            ideal = calculate_dcg(relevant[:k], [1 for i in range(k)])\n",
    "            \n",
    "            \n",
    "            dcg = calculate_dcg(reterived[:k], [1 if i in relevant else 0 for i in reterived][:k])\n",
    "            result += (dcg / ideal)\n",
    "        result = result / num_of_queries\n",
    "    else:\n",
    "        reterived, relevant = get_reterived_and_relevant(query_id)\n",
    "        ideal = calculate_dcg(relevant, [1 for i in range(len(relevant))])\n",
    "        #k = length relevant, r_i > 0 for all docs in relevant\n",
    "        k = len(relevant)\n",
    "        dcg = calculate_dcg(reterived, [1 if i in relevant else 0 for i in reterived][:k])\n",
    "        result = dcg / ideal\n",
    "        \n",
    "        \n",
    "                  \n",
    "    print('NDCG is: ' + str(result))\n",
    "    return result\n",
    "\n",
    "#shomare 18 --> خاورمینا\n",
    "\n",
    "R_Precision('all', 'ltc-lnc')\n",
    "F_measure('all', 'ltc-lnc')\n",
    "MAP('all', 'ltc-lnc')\n",
    "NDCG('all', 'ltc-lnc')\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1Spring99_ژخحغ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
